{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, _data_path: str, _label_col: str):\n",
    "        self.label_col = _label_col\n",
    "        self.data = pd.read_csv(_data_path)\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def clean_data(self):\n",
    "        self.data = self.data.dropna(subset=[self.label_col])\n",
    "        self.data = self.data.drop_duplicates()\n",
    "\n",
    "    def encode_data(self):\n",
    "        label_encoder = LabelEncoder()\n",
    "        for column in self.x.columns:\n",
    "            if self.x[column].dtype == 'object':\n",
    "                if len(self.x[column].unique()) <= 2:\n",
    "                    self.x[column] = label_encoder.fit_transform(self.x[column])\n",
    "                else:\n",
    "                    self.x = pd.get_dummies(self.x, columns=[column])\n",
    "    \n",
    "    def scale_data(self, _mode = 'standard'):\n",
    "        numerical_columns = []\n",
    "        for column in self.x.columns:\n",
    "            if self.x[column].dtype not in [type(object), type(bool)]:\n",
    "                numerical_columns.append(column)\n",
    "        if _mode == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif _mode == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        self.x[numerical_columns] = scaler.fit_transform(self.x[numerical_columns])\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        if self.x == None and self.y == None:\n",
    "            self.clean_data()\n",
    "            self.x = self.data.drop(columns=[self.label_col])\n",
    "            self.y = self.data[self.label_col]\n",
    "            self.y = LabelEncoder().fit_transform(self.y)\n",
    "            self.encode_data()\n",
    "            self.scale_data()\n",
    "        return np.array(self.x), np.array(self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelcoPreprocessor(Preprocessor):\n",
    "    def __init__(self, _data_path: str, _label_col: str):\n",
    "        super().__init__(_data_path, _label_col)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data = self.data.drop(columns=['customerID'])\n",
    "        self.data['TotalCharges'] = pd.to_numeric(self.data['TotalCharges'], errors='coerce')\n",
    "        self.data = self.data.dropna(subset=['TotalCharges'])\n",
    "        return super().preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.__w = np.zeros(self.x.shape[1])\n",
    "    \n",
    "    def __h(self, _x: np.ndarray):\n",
    "        assert _x.shape[1] == self.__w.size\n",
    "        return 1 / (1 + np.exp(-_x @ self.__w))\n",
    "    \n",
    "    def __BGD(self, _lr: float, _epoch: int):\n",
    "        for _ in range(_epoch):\n",
    "            self.__w -= _lr * self.x.T @ (self.__h(self.x) - self.y) / self.y.size\n",
    "    \n",
    "    def __SGD(self, _lr: float, _epoch: int):\n",
    "        for _ in range(_epoch):\n",
    "            for i in range(self.y.size):\n",
    "                self.__w -= _lr * self.x[i].T * (self.__h(self.x)[i] - self.y[i])\n",
    "    \n",
    "    def __miniBGD(self, _lr: float, _epoch: int, _batch_size: int, _verbose: bool):\n",
    "        for ep in range(_epoch):\n",
    "            for i in range(0, self.y.size, _batch_size):\n",
    "                self.__w -= _lr * self.x[i:i+_batch_size].T @ (self.__h(self.x)[i:i+_batch_size] - self.y[i:i+_batch_size]) / _batch_size\n",
    "                if _verbose:\n",
    "                    print(f'Epoch {ep+1}/{_epoch} | Batch {i//_batch_size+1}/{self.y.size//_batch_size} - Loss: {self.__negative_log_likelihood(self.x[i:i+_batch_size], self.y[i:i+_batch_size])}')\n",
    "            metrics = self.__calculate_metrics(self.x, self.y)\n",
    "            if _verbose:\n",
    "                print(f'Epoch {ep+1}/{_epoch} - Loss: {self.__negative_log_likelihood(self.x, self.y)} | Accuracy: {metrics[\"accuracy\"]} | Sensitivity: {metrics[\"sensitivity\"]} | Specificity: {metrics[\"specificity\"]} | Precision: {metrics[\"precision\"]} | F1: {metrics[\"f1\"]} | AUROC: {metrics[\"auroc\"]} | AUPR: {metrics[\"aupr\"]}')\n",
    "    \n",
    "    def __calculate_confusion_matrix(self, _y_pred: np.ndarray, _y_true: np.ndarray):\n",
    "        tp = np.sum((_y_pred == 1) & (_y_true == 1))\n",
    "        tn = np.sum((_y_pred == 0) & (_y_true == 0))\n",
    "        fp = np.sum((_y_pred == 1) & (_y_true == 0))\n",
    "        fn = np.sum((_y_pred == 0) & (_y_true == 1))\n",
    "        return tp, tn, fp, fn\n",
    "    \n",
    "    def __calculate_aupr(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fp == 0:\n",
    "                precision.append(1)\n",
    "            else:\n",
    "                precision.append(tp / (tp + fp))\n",
    "            if tp + fn == 0:\n",
    "                recall.append(1)\n",
    "            else:\n",
    "                recall.append(tp / (tp + fn))\n",
    "        aupr = 0\n",
    "        for i in range(1, len(precision)):\n",
    "            aupr += (recall[i] - recall[i-1]) * precision[i]\n",
    "        return aupr\n",
    "\n",
    "    def __calculate_auroc(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fn == 0:\n",
    "                tpr.append(1)\n",
    "            else:\n",
    "                tpr.append(tp / (tp + fn))\n",
    "            if tn + fp == 0:\n",
    "                fpr.append(1)\n",
    "            else:\n",
    "                fpr.append(fp / (tn + fp))\n",
    "        auroc = 0\n",
    "        for i in range(1, len(fpr)):\n",
    "            auroc += (fpr[i] - fpr[i-1]) * tpr[i]\n",
    "        return auroc\n",
    "    \n",
    "    def __calculate_metrics(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        y_true = _y \n",
    "        y_prob = self.__h(_x)\n",
    "        y_pred = self.predict(_x)\n",
    "        tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, y_true)\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "        if tp + fn == 0:\n",
    "            metrics['sensitivity'] = 1\n",
    "        else:\n",
    "            metrics['sensitivity'] = tp / (tp + fn)\n",
    "        if tn + fp == 0:\n",
    "            metrics['specificity'] = 1\n",
    "        else:\n",
    "            metrics['specificity'] = tn / (tn + fp)\n",
    "        if tp + fp == 0:\n",
    "            metrics['precision'] = 1\n",
    "        else:\n",
    "            metrics['precision'] = tp / (tp + fp)\n",
    "        metrics['f1'] = 2 * metrics['precision'] * metrics['sensitivity'] / (metrics['precision'] + metrics['sensitivity'])\n",
    "        metrics['auroc'] = self.__calculate_auroc(y_prob, y_true)\n",
    "        metrics['aupr'] = self.__calculate_aupr(y_prob, y_true)\n",
    "        return metrics\n",
    "    \n",
    "    def __negative_log_likelihood(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        return -np.sum(_y * np.log(self.__h(_x)) + (1 - _y) * np.log(1 - self.__h(_x))) / _y.size\n",
    "    \n",
    "    \n",
    "    def fit(self, _lr: float = 0.01, _epoch: int = 3, _batch_size: int = 1, _verbose: bool = False):\n",
    "        self.__miniBGD(_lr, _epoch, _batch_size, _verbose)\n",
    "        return self.__calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, _x: np.ndarray, _threshold: float = 0.5):\n",
    "        p = self.__h(_x)\n",
    "        return np.where(p > _threshold, 1, 0)\n",
    "    \n",
    "    def test(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        return self.__calculate_metrics(_x, _y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TelcoPreprocessor('./Telco-Customer-Churn.csv', 'Churn')\n",
    "x, y = tp.preprocess_data()\n",
    "print(x.shape, y.shape)\n",
    "print(y[:10])\n",
    "lr = LogisticRegression(x, y)\n",
    "print(lr.fit(_lr = 0.01, _epoch = 50, _batch_size = 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingEnsemble:\n",
    "    def __init__(self, _x: np.ndarray, _y: np.ndarray, _n_estimators: int = 9, _lr: float = 0.01, _epoch: int = 50, _batch_size: int = 500):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.lr = _lr\n",
    "        self.epoch = _epoch\n",
    "        self.batch_size = _batch_size\n",
    "        self.n_estimators = _n_estimators\n",
    "        self.estimators = []\n",
    "    \n",
    "    def __bootstrap_sample(self):\n",
    "        indices = np.random.choice(self.y.size, size=self.y.size, replace=True)\n",
    "        return self.x[indices], self.y[indices]\n",
    "    \n",
    "    def __fit_estimators(self):\n",
    "        for _ in range(self.n_estimators):\n",
    "            x_sample, y_sample = self.__bootstrap_sample()\n",
    "            model = LogisticRegression(x_sample, y_sample)\n",
    "            model.fit(_lr = self.lr, _epoch = self.epoch, _batch_size = self.batch_size)\n",
    "            self.estimators.append(model)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
