{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_path: str):\n",
    "    data = pd.read_csv(_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(_data: pd.DataFrame):\n",
    "    _data = _data.dropna()\n",
    "    _data = _data.drop_duplicates()\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(_data: pd.DataFrame):\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in _data.columns:\n",
    "        if _data[column].dtype == 'object':\n",
    "            if len(_data[column].unique()) <= 2:\n",
    "                _data[column] = label_encoder.fit_transform(_data[column])\n",
    "            else:\n",
    "                _data = pd.get_dummies(_data, columns=[column])\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(_data: pd.DataFrame, _mode: str = 'minmax'):\n",
    "    # normalize only numerical columns\n",
    "    numerical_columns = []\n",
    "    for column in _data.columns:\n",
    "        if _data[column].dtype != 'object':\n",
    "            numerical_columns.append(column)\n",
    "    if _mode == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif _mode == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    _data[numerical_columns] = scaler.fit_transform(_data[numerical_columns])\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_label(_data: pd.DataFrame, _label: str):\n",
    "    label = _data[_label]\n",
    "    features = _data.drop(columns=[_label])\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(_features: pd.DataFrame):\n",
    "    _features = encode(_features)\n",
    "    _features = normalize(_features)\n",
    "    return _features\n",
    "\n",
    "def preprocess_label(_label: pd.Series):\n",
    "    label_encoder = LabelEncoder()\n",
    "    _label = label_encoder.fit_transform(_label)\n",
    "    return _label\n",
    "\n",
    "def preprocess_data(_data: pd.DataFrame, _label: str):\n",
    "    _data = clean(_data)\n",
    "    features, label = separate_label(_data, _label)\n",
    "    x = preprocess_features(features)\n",
    "    y = preprocess_label(label)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_telco(_path: str):\n",
    "    data = load_data(_path)\n",
    "    x, y = preprocess_data(data, 'Churn')\n",
    "    return x, y\n",
    "\n",
    "x, y = preprocess_telco('./Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 13613) (7043,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.__w = np.zeros(self.x.shape[1])\n",
    "    \n",
    "    def __h(self, _x: np.ndarray):\n",
    "        assert _x.shape[1] == self.__w.size\n",
    "        return 1 / (1 + np.exp(-_x @ self.__w))\n",
    "    \n",
    "    def __BGD(self, _lr: float, _epoch: int):\n",
    "        for _ in range(_epoch):\n",
    "            self.__w -= _lr * self.x.T @ (self.__h(self.x) - self.y) / self.y.size\n",
    "    \n",
    "    def __SGD(self, _lr: float, _epoch: int):\n",
    "        for _ in range(_epoch):\n",
    "            for i in range(self.y.size):\n",
    "                self.__w -= _lr * self.x[i].T * (self.__h(self.x)[i] - self.y[i])\n",
    "    \n",
    "    def __miniBGD(self, _lr: float, _epoch: int, _batch_size: int):\n",
    "        for ep in range(_epoch):\n",
    "            for i in range(0, self.y.size, _batch_size):\n",
    "                self.__w -= _lr * self.x[i:i+_batch_size].T @ (self.__h(self.x)[i:i+_batch_size] - self.y[i:i+_batch_size]) / _batch_size\n",
    "                print(f'Epoch {ep+1}/{_epoch} | Batch {i//_batch_size+1}/{self.y.size//_batch_size} - Loss: {self.__negative_log_likelihood(self.x[i:i+_batch_size], self.y[i:i+_batch_size])}')\n",
    "            metrics = self.__calculate_metrics(self.x, self.y)\n",
    "            print(f'Epoch {ep+1}/{_epoch} - Loss: {self.__negative_log_likelihood(self.x, self.y)} | Accuracy: {metrics[\"accuracy\"]} | Sensitivity: {metrics[\"sensitivity\"]} | Specificity: {metrics[\"specificity\"]} | Precision: {metrics[\"precision\"]} | F1: {metrics[\"f1\"]} | AUROC: {metrics[\"auroc\"]} | AUPR: {metrics[\"aupr\"]}')\n",
    "    \n",
    "    def __calculate_confusion_matrix(self, _y_pred: np.ndarray, _y_true: np.ndarray):\n",
    "        tp = np.sum((_y_pred == 1) & (_y_true == 1))\n",
    "        tn = np.sum((_y_pred == 0) & (_y_true == 0))\n",
    "        fp = np.sum((_y_pred == 1) & (_y_true == 0))\n",
    "        fn = np.sum((_y_pred == 0) & (_y_true == 1))\n",
    "        return tp, tn, fp, fn\n",
    "    \n",
    "    def __calculate_aupr(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fp == 0:\n",
    "                precision.append(1)\n",
    "            else:\n",
    "                precision.append(tp / (tp + fp))\n",
    "            if tp + fn == 0:\n",
    "                recall.append(1)\n",
    "            else:\n",
    "                recall.append(tp / (tp + fn))\n",
    "        aupr = 0\n",
    "        for i in range(1, len(precision)):\n",
    "            aupr += (recall[i] - recall[i-1]) * precision[i]\n",
    "        return aupr\n",
    "\n",
    "    def __calculate_auroc(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fn == 0:\n",
    "                tpr.append(1)\n",
    "            else:\n",
    "                tpr.append(tp / (tp + fn))\n",
    "            if tn + fp == 0:\n",
    "                fpr.append(1)\n",
    "            else:\n",
    "                fpr.append(fp / (tn + fp))\n",
    "        auroc = 0\n",
    "        for i in range(1, len(fpr)):\n",
    "            auroc += (fpr[i] - fpr[i-1]) * tpr[i]\n",
    "        return auroc\n",
    "    \n",
    "    def __calculate_metrics(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        y_true = _y \n",
    "        y_prob = self.__h(_x)\n",
    "        y_pred = self.predict(_x)\n",
    "        tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, y_true)\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "        if tp + fn == 0:\n",
    "            metrics['sensitivity'] = 1\n",
    "        else:\n",
    "            metrics['sensitivity'] = tp / (tp + fn)\n",
    "        if tn + fp == 0:\n",
    "            metrics['specificity'] = 1\n",
    "        else:\n",
    "            metrics['specificity'] = tn / (tn + fp)\n",
    "        if tp + fp == 0:\n",
    "            metrics['precision'] = 1\n",
    "        else:\n",
    "            metrics['precision'] = tp / (tp + fp)\n",
    "        metrics['f1'] = 2 * metrics['precision'] * metrics['sensitivity'] / (metrics['precision'] + metrics['sensitivity'])\n",
    "        metrics['auroc'] = self.__calculate_auroc(y_prob, y_true)\n",
    "        metrics['aupr'] = self.__calculate_aupr(y_prob, y_true)\n",
    "        return metrics\n",
    "    \n",
    "    def __negative_log_likelihood(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        return -np.sum(_y * np.log(self.__h(_x)) + (1 - _y) * np.log(1 - self.__h(_x))) / _y.size\n",
    "    \n",
    "    \n",
    "    def fit(self, _lr: float, _epoch: int, _batch_size: int = 1):\n",
    "        self.__miniBGD(_lr, _epoch, _batch_size)\n",
    "        return self.__calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, _x: np.ndarray, _threshold: float = 0.5):\n",
    "        p = self.__h(_x)\n",
    "        return np.where(p > _threshold, 1, 0)\n",
    "    \n",
    "    def test(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        return self.__calculate_metrics(_x, _y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Batch 1/14 - Loss: 0.6895932253045496\n",
      "Epoch 1/10 | Batch 2/14 - Loss: 0.6859629641264079\n",
      "Epoch 1/10 | Batch 3/14 - Loss: 0.6838905237275854\n",
      "Epoch 1/10 | Batch 4/14 - Loss: 0.6796088304508049\n",
      "Epoch 1/10 | Batch 5/14 - Loss: 0.6759149806171987\n",
      "Epoch 1/10 | Batch 6/14 - Loss: 0.6733608608460622\n",
      "Epoch 1/10 | Batch 7/14 - Loss: 0.6718499475252069\n",
      "Epoch 1/10 | Batch 8/14 - Loss: 0.6707242614480841\n",
      "Epoch 1/10 | Batch 9/14 - Loss: 0.6634996610864116\n",
      "Epoch 1/10 | Batch 10/14 - Loss: 0.6630093587706335\n",
      "Epoch 1/10 | Batch 11/14 - Loss: 0.6612308712728877\n",
      "Epoch 1/10 | Batch 12/14 - Loss: 0.660104067142682\n",
      "Epoch 1/10 | Batch 13/14 - Loss: 0.6552880953219625\n",
      "Epoch 1/10 | Batch 14/14 - Loss: 0.6558634217391023\n",
      "Epoch 1/10 | Batch 15/14 - Loss: 0.6585263284330837\n",
      "Epoch 1/10 - Loss: 0.6532128756858274 | Accuracy: 0.7346301292063041 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.7593339790279582 | AUPR: -0.47906775161065757\n",
      "Epoch 2/10 | Batch 1/14 - Loss: 0.6494308742571816\n",
      "Epoch 2/10 | Batch 2/14 - Loss: 0.6458940862588564\n",
      "Epoch 2/10 | Batch 3/14 - Loss: 0.6487793193118575\n",
      "Epoch 2/10 | Batch 4/14 - Loss: 0.6415870308257585\n",
      "Epoch 2/10 | Batch 5/14 - Loss: 0.6380344034864089\n",
      "Epoch 2/10 | Batch 6/14 - Loss: 0.6374297619375747\n",
      "Epoch 2/10 | Batch 7/14 - Loss: 0.6389383192238522\n",
      "Epoch 2/10 | Batch 8/14 - Loss: 0.6406568092578039\n",
      "Epoch 2/10 | Batch 9/14 - Loss: 0.6283547307408615\n",
      "Epoch 2/10 | Batch 10/14 - Loss: 0.631380537714718\n",
      "Epoch 2/10 | Batch 11/14 - Loss: 0.631204408718977\n",
      "Epoch 2/10 | Batch 12/14 - Loss: 0.6317945645799321\n",
      "Epoch 2/10 | Batch 13/14 - Loss: 0.6254602535061952\n",
      "Epoch 2/10 | Batch 14/14 - Loss: 0.6287907405115458\n",
      "Epoch 2/10 | Batch 15/14 - Loss: 0.6338433138375041\n",
      "Epoch 2/10 - Loss: 0.6242833504016743 | Accuracy: 0.7346301292063041 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.770382761235902 | AUPR: -0.4942988687307598\n",
      "Epoch 3/10 | Batch 1/14 - Loss: 0.6202383619316806\n",
      "Epoch 3/10 | Batch 2/14 - Loss: 0.6166498198404454\n",
      "Epoch 3/10 | Batch 3/14 - Loss: 0.6232967463326823\n",
      "Epoch 3/10 | Batch 4/14 - Loss: 0.6136493837932355\n",
      "Epoch 3/10 | Batch 5/14 - Loss: 0.610172029970655\n",
      "Epoch 3/10 | Batch 6/14 - Loss: 0.610783503954181\n",
      "Epoch 3/10 | Batch 7/14 - Loss: 0.6146675870176005\n",
      "Epoch 3/10 | Batch 8/14 - Loss: 0.6187763773876394\n",
      "Epoch 3/10 | Batch 9/14 - Loss: 0.6023442309752468\n",
      "Epoch 3/10 | Batch 10/14 - Loss: 0.6080456528851309\n",
      "Epoch 3/10 | Batch 11/14 - Loss: 0.6091435955910067\n",
      "Epoch 3/10 | Batch 12/14 - Loss: 0.6109930241969582\n",
      "Epoch 3/10 | Batch 13/14 - Loss: 0.6033309586309349\n",
      "Epoch 3/10 | Batch 14/14 - Loss: 0.6086340633339761\n",
      "Epoch 3/10 | Batch 15/14 - Loss: 0.6156707379235502\n",
      "Epoch 3/10 - Loss: 0.6027118184443386 | Accuracy: 0.7346301292063041 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.7790730621457281 | AUPR: -0.5073440219139467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(x, y)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[28], line 105\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, _lr, _epoch, _batch_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, _lr: \u001b[38;5;28mfloat\u001b[39m, _epoch: \u001b[38;5;28mint\u001b[39m, _batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__miniBGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__calculate_metrics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n",
      "Cell \u001b[0;32mIn[28], line 23\u001b[0m, in \u001b[0;36mLogisticRegression.__miniBGD\u001b[0;34m(self, _lr, _epoch, _batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_epoch):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msize, _batch_size):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m _lr \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[i:i\u001b[38;5;241m+\u001b[39m_batch_size]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__h\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[i:i\u001b[38;5;241m+\u001b[39m_batch_size] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i:i\u001b[38;5;241m+\u001b[39m_batch_size]) \u001b[38;5;241m/\u001b[39m _batch_size\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m_batch_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__negative_log_likelihood(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[i:i\u001b[38;5;241m+\u001b[39m_batch_size],\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[i:i\u001b[38;5;241m+\u001b[39m_batch_size])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__calculate_metrics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m, in \u001b[0;36mLogisticRegression.__h\u001b[0;34m(self, _x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__h\u001b[39m(\u001b[38;5;28mself\u001b[39m, _x: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__w\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__w\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(x, y)\n",
    "print(lr.fit(0.01, 10, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
