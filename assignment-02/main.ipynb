{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, _data_path: str, _label_col: str):\n",
    "        self.label_col = _label_col\n",
    "        self.data = pd.read_csv(_data_path)\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def clean_data(self):\n",
    "        self.data = self.data.dropna(subset=[self.label_col])\n",
    "        self.data = self.data.drop_duplicates()\n",
    "\n",
    "    def encode_data(self):\n",
    "        label_encoder = LabelEncoder()\n",
    "        for column in self.x.columns:\n",
    "            if self.x[column].dtype == 'object':\n",
    "                if len(self.x[column].unique()) <= 2:\n",
    "                    self.x[column] = label_encoder.fit_transform(self.x[column])\n",
    "                else:\n",
    "                    self.x = pd.get_dummies(self.x, columns=[column])\n",
    "    \n",
    "    def scale_data(self, _mode = 'standard'):\n",
    "        numerical_columns = []\n",
    "        for column in self.x.columns:\n",
    "            if self.x[column].dtype not in [type(object), type(bool)]:\n",
    "                numerical_columns.append(column)\n",
    "        if _mode == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif _mode == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        self.x[numerical_columns] = scaler.fit_transform(self.x[numerical_columns])\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        if self.x == None and self.y == None:\n",
    "            self.clean_data()\n",
    "            self.x = self.data.drop(columns=[self.label_col])\n",
    "            self.y = self.data[self.label_col]\n",
    "            self.y = LabelEncoder().fit_transform(self.y)\n",
    "            self.encode_data()\n",
    "            self.scale_data()\n",
    "        return np.array(self.x), np.array(self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelcoPreprocessor(Preprocessor):\n",
    "    def __init__(self, _data_path: str, _label_col: str):\n",
    "        super().__init__(_data_path, _label_col)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data = self.data.drop(columns=['customerID'])\n",
    "        self.data['TotalCharges'] = pd.to_numeric(self.data['TotalCharges'], errors='coerce')\n",
    "        self.data = self.data.dropna(subset=['TotalCharges'])\n",
    "        return super().preprocess_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, lr: float, epoch: int, batch_size: int, verbose: bool):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x: np.ndarray, threshold: float):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, x: np.ndarray, y: np.ndarray):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(Model):\n",
    "    @abstractmethod\n",
    "    def h(self, x: np.ndarray):\n",
    "        pass\n",
    "\n",
    "    def __calculate_confusion_matrix(self, _y_pred: np.ndarray, _y_true: np.ndarray):\n",
    "        tp = np.sum((_y_pred == 1) & (_y_true == 1))\n",
    "        tn = np.sum((_y_pred == 0) & (_y_true == 0))\n",
    "        fp = np.sum((_y_pred == 1) & (_y_true == 0))\n",
    "        fn = np.sum((_y_pred == 0) & (_y_true == 1))\n",
    "        return tp, tn, fp, fn\n",
    "    \n",
    "    def __calculate_aupr(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        precision = []\n",
    "        recall = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fp == 0:\n",
    "                precision.append(1)\n",
    "            else:\n",
    "                precision.append(tp / (tp + fp))\n",
    "            if tp + fn == 0:\n",
    "                recall.append(1)\n",
    "            else:\n",
    "                recall.append(tp / (tp + fn))\n",
    "        aupr = 0\n",
    "        for i in range(1, len(precision)):\n",
    "            aupr += (recall[i] - recall[i-1]) * precision[i]\n",
    "        return aupr\n",
    "\n",
    "    def __calculate_auroc(self, _y_prob: np.ndarray, _y_true: np.ndarray):\n",
    "        thresholds = np.sort(np.unique(_y_prob))\n",
    "        tpr = []\n",
    "        fpr = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = np.where(_y_prob > threshold, 1, 0)\n",
    "            tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, _y_true)\n",
    "            if tp + fn == 0:\n",
    "                tpr.append(1)\n",
    "            else:\n",
    "                tpr.append(tp / (tp + fn))\n",
    "            if tn + fp == 0:\n",
    "                fpr.append(1)\n",
    "            else:\n",
    "                fpr.append(fp / (tn + fp))\n",
    "        auroc = 0\n",
    "        for i in range(1, len(fpr)):\n",
    "            auroc += (fpr[i] - fpr[i-1]) * tpr[i]\n",
    "        return auroc\n",
    "    \n",
    "    def calculate_metrics(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        y_true = _y \n",
    "        y_prob = self.h(_x)\n",
    "        y_pred = self.predict(_x)\n",
    "        tp, tn, fp, fn = self.__calculate_confusion_matrix(y_pred, y_true)\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "        if tp + fn == 0:\n",
    "            metrics['sensitivity'] = 1\n",
    "        else:\n",
    "            metrics['sensitivity'] = tp / (tp + fn)\n",
    "        if tn + fp == 0:\n",
    "            metrics['specificity'] = 1\n",
    "        else:\n",
    "            metrics['specificity'] = tn / (tn + fp)\n",
    "        if tp + fp == 0:\n",
    "            metrics['precision'] = 1\n",
    "        else:\n",
    "            metrics['precision'] = tp / (tp + fp)\n",
    "        metrics['f1'] = 2 * metrics['precision'] * metrics['sensitivity'] / (metrics['precision'] + metrics['sensitivity'])\n",
    "        metrics['auroc'] = self.__calculate_auroc(y_prob, y_true)\n",
    "        metrics['aupr'] = self.__calculate_aupr(y_prob, y_true)\n",
    "        return metrics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(BinaryClassifier):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.__w = np.zeros(self.x.shape[1])\n",
    "   \n",
    "    def __miniBGD(self, _lr: float, _epoch: int, _batch_size: int, _verbose: bool):\n",
    "        for ep in range(_epoch):\n",
    "            for i in range(0, self.y.size, _batch_size):\n",
    "                self.__w -= _lr * self.x[i:i+_batch_size].T @ (self.h(self.x)[i:i+_batch_size] - self.y[i:i+_batch_size]) / _batch_size\n",
    "                if _verbose:\n",
    "                    print(f'Epoch {ep+1}/{_epoch} | Batch {i//_batch_size+1}/{self.y.size//_batch_size} - Loss: {self.__negative_log_likelihood(self.x[i:i+_batch_size], self.y[i:i+_batch_size])}')\n",
    "            metrics = self.calculate_metrics(self.x, self.y)\n",
    "            if _verbose:\n",
    "                print(f'Epoch {ep+1}/{_epoch} - Loss: {self.__negative_log_likelihood(self.x, self.y)} | Accuracy: {metrics[\"accuracy\"]} | Sensitivity: {metrics[\"sensitivity\"]} | Specificity: {metrics[\"specificity\"]} | Precision: {metrics[\"precision\"]} | F1: {metrics[\"f1\"]} | AUROC: {metrics[\"auroc\"]} | AUPR: {metrics[\"aupr\"]}')\n",
    "    \n",
    "    def __negative_log_likelihood(self, _x: np.ndarray, _y: np.ndarray):\n",
    "        return -np.sum(_y * np.log(self.h(_x)) + (1 - _y) * np.log(1 - self.h(_x))) / _y.size\n",
    "\n",
    "    def h(self, x: np.ndarray):\n",
    "        assert x.shape[1] == self.__w.size\n",
    "        return 1 / (1 + np.exp(-x @ self.__w))\n",
    "    \n",
    "    def fit(self, lr: float = 0.01, epoch: int = 3, batch_size: int = 1, verbose: bool = False):\n",
    "        self.__miniBGD(lr, epoch, batch_size, verbose)\n",
    "        return self.calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, x: np.ndarray, threshold: float = 0.5):\n",
    "        p = self.h(x)\n",
    "        return np.where(p > threshold, 1, 0)\n",
    "    \n",
    "    def test(self, x: np.ndarray, y: np.ndarray):\n",
    "        return self.calculate_metrics(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrapper:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, n_estimators: int = 9):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = []\n",
    "    \n",
    "    def __bootstrap_sample(self):\n",
    "        indices = np.random.choice(self.y.size, size=self.y.size, replace=True)\n",
    "        return self.x[indices], self.y[indices]\n",
    "    \n",
    "    def __fit_estimators(self, _lr: float, _epoch: int, _batch_size: int, _verbose: bool):\n",
    "        self.estimators = []\n",
    "        for i in range(self.n_estimators):\n",
    "            if _verbose:\n",
    "                print(f'\\nFitting estimator {i+1}/{self.n_estimators}')\n",
    "            x_sample, y_sample = self.__bootstrap_sample()\n",
    "            estimator = LogisticRegressor(x_sample, y_sample)\n",
    "            estimator.fit(lr = _lr, epoch = _epoch, batch_size = _batch_size, verbose = _verbose)\n",
    "            self.estimators.append(estimator)\n",
    "    \n",
    "    def get_estimators(self, lr: float = 0.01, epoch: int = 3, batch_size: int = 1, verbose: bool = False):\n",
    "        self.__fit_estimators(lr, epoch, batch_size, verbose)\n",
    "        return self.estimators\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEnsembler(BinaryClassifier):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, n_estimators: int = 9):\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        self.n_estimators = n_estimators\n",
    "        self.__bootstrapper = Bootstrapper(self.x, self.y, self.n_estimators)\n",
    "        self.__estimators = []\n",
    "    \n",
    "    def h(self, x: np.ndarray):\n",
    "        assert self.__estimators != []\n",
    "        return np.mean([estimator.h(x) for estimator in self.__estimators], axis=0)\n",
    "    \n",
    "    def fit(self, lr: float = 0.01, epoch: int = 3, batch_size: int = 1, verbose: bool = False):\n",
    "        self.__estimators = self.__bootstrapper.get_estimators(lr, epoch, batch_size, verbose)\n",
    "        return self.calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, x: np.ndarray, threshold: float = 0.5):\n",
    "        p = self.h(x)\n",
    "        return np.where(p > threshold, 1, 0)\n",
    "    \n",
    "    def test(self, x: np.ndarray, y: np.ndarray):\n",
    "        return self.calculate_metrics(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingEnsembler(BinaryClassifier):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, n_estimators: int = 9):\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        self.n_estimators = n_estimators\n",
    "        self.__bootstrapper = Bootstrapper(self.x, self.y, self.n_estimators)\n",
    "        self.__estimators = []\n",
    "    \n",
    "    def h(self, x: np.ndarray):\n",
    "        assert self.__estimators != []\n",
    "        return np.median([estimator.h(x) for estimator in self.__estimators], axis=0)\n",
    "    \n",
    "    def fit(self, lr: float = 0.01, epoch: int = 3, batch_size: int = 1, verbose: bool = False):\n",
    "        self.__estimators = self.__bootstrapper.get_estimators(lr, epoch, batch_size, verbose)\n",
    "        return self.calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, x: np.ndarray, threshold: float = 0.5):\n",
    "        votes = np.array([estimator.predict(x) for estimator in self.__estimators])\n",
    "        return np.where(np.sum(votes, axis=0) > self.n_estimators / 2, 1, 0) \n",
    "    \n",
    "    def test(self, x: np.ndarray, y: np.ndarray):\n",
    "        return self.calculate_metrics(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingEnsembler(BinaryClassifier):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, n_estimators: int = 9):\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        self.n_estimators = n_estimators\n",
    "        self.__bootstrapper = Bootstrapper(self.x, self.y, self.n_estimators)\n",
    "        self.__base_learners = []\n",
    "        self.__meta_x = None\n",
    "        self.__meta_y = None\n",
    "        self.__meta_learner = None\n",
    "    \n",
    "    def __generate_metaset(self):\n",
    "        self.__meta_x = np.array([base_learner.h(self.x) for base_learner in self.__base_learners]).T\n",
    "        self.__meta_y = self.y\n",
    "    \n",
    "    def __fit_meta_learner(self, _lr: float, _epoch: int, _batch_size: int, _verbose: bool):\n",
    "        self.__generate_metaset()\n",
    "        self.__meta_learner = LogisticRegressor(self.__meta_x, self.__meta_y)\n",
    "        if _verbose:\n",
    "            print('\\nFitting meta learner')\n",
    "        self.__meta_learner.fit(lr = _lr, epoch = _epoch, batch_size = _batch_size, verbose = _verbose)\n",
    "\n",
    "    def h(self, x: np.ndarray):\n",
    "        assert self.__base_learners != [] and self.__meta_learner != None\n",
    "        meta_x = np.array([base_learner.h(x) for base_learner in self.__base_learners]).T\n",
    "        return self.__meta_learner.h(meta_x)\n",
    "    \n",
    "    def fit(self, lr: float = 0.01, epoch: int = 3, batch_size: int = 1, verbose: bool = False):\n",
    "        self.__base_learners = self.__bootstrapper.get_estimators(lr, epoch, batch_size, verbose)\n",
    "        self.__fit_meta_learner(lr, epoch, batch_size, verbose)\n",
    "        return self.calculate_metrics(self.x, self.y)\n",
    "    \n",
    "    def predict(self, x: np.ndarray, threshold: float = 0.5):\n",
    "        meta_x = np.array([base_learner.h(x) for base_learner in self.__base_learners]).T\n",
    "        return self.__meta_learner.predict(meta_x)\n",
    "    \n",
    "    def test(self, x: np.ndarray, y: np.ndarray):\n",
    "        return self.calculate_metrics(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7010, 40) (7010,)\n",
      "[0 0 1 0 1 1 0 0 1 0]\n",
      "\n",
      "Fitting estimator 1/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6882897410014102\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.686590431054059\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6820442231906936\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6796879127625499\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6761480758484647\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6748086171648982\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.670702251945238\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6688926692890517\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6668003429193714\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6647940668313185\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6625042107564314\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6620706830390891\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6647760338021278\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6644287139448833\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6799427108793037\n",
      "Epoch 1/3 - Loss: 0.6573899454689763 | Accuracy: 0.6597717546362339 | Sensitivity: 0.8728579325594251 | Specificity: 0.5856566044991348 | Precision: 0.42287091590787357 | F1: 0.5697275843406098 | AUROC: -0.8181560100967064 | AUPR: -0.5966761293844186\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6458984738121575\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6552340823573202\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6469404937115327\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6484131844669249\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6447900329024494\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6466580405559456\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6424668350370726\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6423891437365833\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6406913917005798\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6403671043817172\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.638525102164712\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6403000544573547\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6466866319339287\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6472291633481152\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6742941639422446\n",
      "Epoch 2/3 - Loss: 0.6356030334184357 | Accuracy: 0.666191155492154 | Sensitivity: 0.8700939745715865 | Specificity: 0.5952701403576235 | Precision: 0.4278336504484914 | F1: 0.5736151603498542 | AUROC: -0.8198223563121787 | AUPR: -0.5984863995096179\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6189566922775218\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6359900731786263\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6244959591423501\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6290998804255256\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6252334823375623\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6288283337036089\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6252023513389988\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6260290401698376\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6238180771750705\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6249971336084971\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6230726612098616\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6264728229174487\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.63528038971972\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6362544330804565\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.6720558460692954\n",
      "Epoch 3/3 - Loss: 0.6214591850318544 | Accuracy: 0.6731811697574893 | Sensitivity: 0.865118850193477 | Specificity: 0.6064218419534705 | Precision: 0.4332779623477298 | F1: 0.5773842464489947 | AUROC: -0.8216886258106811 | AUPR: -0.6003216770486131\n",
      "\n",
      "Fitting estimator 2/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6902362404318856\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6874849574093485\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6836532058680819\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6832132613271577\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6788392404813943\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6713207696175802\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6721679137447947\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6670956509759771\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6700347920497065\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6622423201716472\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6732263689751375\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6664466158945093\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6604970692521829\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.655581125331149\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6642331347598815\n",
      "Epoch 1/3 - Loss: 0.6585036661075552 | Accuracy: 0.6674750356633381 | Sensitivity: 0.8648941942485079 | Specificity: 0.5970582543061738 | Precision: 0.43362350380848746 | F1: 0.5776408769704657 | AUROC: -0.8157125528771476 | AUPR: -0.6071389373772412\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6584286824988596\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.65750431609215\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.650339777662582\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6575279673416975\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6491955949926737\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6352146959224243\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6434683127051838\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6372135705404611\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6476725357241039\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6353618340230712\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6577950006087567\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.646803122744717\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6384285981665982\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6320108558361283\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6479925517805384\n",
      "Epoch 2/3 - Loss: 0.6372648594722176 | Accuracy: 0.6758915834522111 | Sensitivity: 0.861096039066739 | Specificity: 0.6098316237662086 | Precision: 0.440466278101582 | F1: 0.5828130738156445 | AUROC: -0.8178956336389565 | AUPR: -0.6089353957056168\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6388003572305526\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6386460206572546\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6296826914494992\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6415007720468111\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6305549489170801\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6119470248876125\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6248830628351536\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6182645529020214\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6341424552758588\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6184953056141447\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6485210852089942\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.634336466690488\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6241368069356383\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6163667429840805\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.6385750723271061\n",
      "Epoch 3/3 - Loss: 0.6234798660290807 | Accuracy: 0.6794579172610556 | Sensitivity: 0.8562126966901791 | Specificity: 0.6164118443971357 | Precision: 0.44325842696629214 | F1: 0.5841199333703497 | AUROC: -0.8200273638551613 | AUPR: -0.6112634827336055\n",
      "\n",
      "Fitting estimator 3/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6893842744677541\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6873108601419028\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6848260403396532\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6823257688788987\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6786889734688678\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6789432485634296\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6739320767949658\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6701054631924426\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6701301235997661\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6672740291187704\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6650061656578404\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6627414361343149\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6580499145125808\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6583951604251661\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6402160273487008\n",
      "Epoch 1/3 - Loss: 0.6588839584035485 | Accuracy: 0.6686162624821683 | Sensitivity: 0.8643790849673203 | Specificity: 0.5991495941244684 | Precision: 0.4334881180005463 | F1: 0.5774058577405858 | AUROC: -0.8205447170492983 | AUPR: -0.6091571658587055\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6524267798331239\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6579675478097642\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6556698383409364\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6543521532864938\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6493032393000705\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6553697325494701\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6459802050611159\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6414015063911295\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6458609432991618\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6433216311403734\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6409212545040291\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6397552862205461\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6339031738944244\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.637218177866355\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6021496594264615\n",
      "Epoch 2/3 - Loss: 0.6375666587845483 | Accuracy: 0.6754636233951498 | Sensitivity: 0.8616557734204793 | Specificity: 0.6093931194433707 | Precision: 0.4390785456563974 | F1: 0.5817245817245816 | AUROC: -0.8226447302710981 | AUPR: -0.610670874783027\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6284915758087881\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6402836887098424\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6371520737511243\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6370211385761686\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6304759026928525\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6411765177688168\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6274377288702259\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.622366743337315\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.63065644620041\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6283243927653038\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6250135107403817\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6249083305499521\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6180989430749275\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.623715537443293\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.5738717969621958\n",
      "Epoch 3/3 - Loss: 0.6236243021102905 | Accuracy: 0.6796005706134094 | Sensitivity: 0.8567538126361656 | Specificity: 0.6167375338229609 | Precision: 0.4423509561304837 | F1: 0.5834569732937686 | AUROC: -0.8247154786838514 | AUPR: -0.6119795733671597\n",
      "\n",
      "Fitting estimator 4/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6902482392682017\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6873678913219438\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6838133522482057\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6785873768621157\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6774989509941518\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6755680134008167\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6732725660229337\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.668046480639841\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6704986001867701\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6678536177587505\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6594929670547421\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6622811435766228\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6543401320744051\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6643357917893652\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6663973965895411\n",
      "Epoch 1/3 - Loss: 0.6573942572647549 | Accuracy: 0.6640513552068473 | Sensitivity: 0.8683195592286501 | Specificity: 0.5926852743022136 | Precision: 0.4268689057421452 | F1: 0.5723624477937171 | AUROC: -0.8226548625638633 | AUPR: -0.6135810859955857\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6580998421096044\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6559013836212952\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6511311022488916\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6427181880074732\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6466308608320039\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6474841018648483\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6458843166362278\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6384632486132157\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6477677291094982\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6442592893019252\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6322470630263651\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6396618887159669\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6281910282688152\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6477715212197207\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6479642159505496\n",
      "Epoch 2/3 - Loss: 0.635467108963482 | Accuracy: 0.6716119828815977 | Sensitivity: 0.8661157024793389 | Specificity: 0.6036573628488932 | Precision: 0.4329385844120077 | F1: 0.5773044436283511 | AUROC: -0.8246529694530336 | AUPR: -0.6154447903582444\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6385426287016782\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6360844105703777\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6313555964155891\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.620273452466867\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6269261510887446\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6298243770234417\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6280614058162726\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6193251982092931\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6336856104746611\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6288035695825586\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6148378183138047\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6250684732704966\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6108502049800176\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6377976676153546\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.63440319073864\n",
      "Epoch 3/3 - Loss: 0.6211708750251352 | Accuracy: 0.6740370898716119 | Sensitivity: 0.8606060606060606 | Specificity: 0.6088546679499519 | Precision: 0.434613244296049 | F1: 0.577555925309669 | AUROC: -0.8267097256580146 | AUPR: -0.6174134200723241\n",
      "\n",
      "Fitting estimator 5/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6908835173761777\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6883281959756716\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6841779384397305\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6824590227631684\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6778749343948745\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6793216877966831\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6740261059123025\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6712717803065461\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6652371312272912\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6673420766018936\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6682239368971153\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6566391454672531\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6537959393230717\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.655073901565618\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6303393741261368\n",
      "Epoch 1/3 - Loss: 0.6574422324913648 | Accuracy: 0.675320970042796 | Sensitivity: 0.8792735042735043 | Specificity: 0.6010120669521214 | Precision: 0.4453463203463203 | F1: 0.591235632183908 | AUROC: -0.8248796881290031 | AUPR: -0.6050705965775985\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6637337457149871\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6594979908425244\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6502374257923031\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6527725230067059\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6455345790558891\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6554797643110772\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6455548609388814\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6437882847532613\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6341304788334529\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6425557719811963\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6475332024424402\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.628390719991429\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6269786195942155\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6316971853605697\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.5877728622976284\n",
      "Epoch 2/3 - Loss: 0.6355025497513617 | Accuracy: 0.6804564907275321 | Sensitivity: 0.8728632478632479 | Specificity: 0.6103542234332425 | Precision: 0.4493949394939494 | F1: 0.5933188090050835 | AUROC: -0.8266905003110737 | AUPR: -0.6067233794061553\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6482251987914588\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6421750742044401\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6290239015668878\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6337429936070947\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6250942485050005\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6412230087583062\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6271772301410062\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6265062974626888\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.613232354781188\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6263280440167296\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6349461209488124\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.609798881195951\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.609640776948534\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6166306408480513\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.5574937602457475\n",
      "Epoch 3/3 - Loss: 0.6212726520694776 | Accuracy: 0.6845934379457917 | Sensitivity: 0.8669871794871795 | Specificity: 0.6181393538341767 | Precision: 0.45271966527196655 | F1: 0.5948323254535459 | AUROC: -0.8286160932618686 | AUPR: -0.6089667826012031\n",
      "\n",
      "Fitting estimator 6/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6887324062282162\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6841843366456021\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6820469893542803\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6772375815731229\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6761007882651212\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6710315921565847\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6709858711910718\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6684861301230643\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6670884830287843\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6693569142710117\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6600672482912744\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6613704231341474\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6539448653736181\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6554780786685573\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6466370198547162\n",
      "Epoch 1/3 - Loss: 0.6538905567034695 | Accuracy: 0.6841654778887304 | Sensitivity: 0.8715846994535519 | Specificity: 0.6179536679536679 | Precision: 0.4462786793508674 | F1: 0.5903034789045151 | AUROC: -0.8324248370150017 | AUPR: -0.6312403496825109\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6468096567719147\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6423883451443866\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6468541744542796\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6395513445638668\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6444555369506538\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6367434640313899\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.642597255239101\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6396364144871874\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6409659323928404\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6482596864867658\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6330899437588304\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6381188806155053\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6271346758075204\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.632628611550231\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6220151066278006\n",
      "Epoch 2/3 - Loss: 0.6296204863753738 | Accuracy: 0.6878744650499287 | Sensitivity: 0.8683060109289618 | Specificity: 0.6241312741312741 | Precision: 0.44937782805429866 | F1: 0.592247484159523 | AUROC: -0.834422326307575 | AUPR: -0.6332537867479764\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6205040592508494\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6158427356913831\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6247681506489421\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6149340575048824\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6245592654919216\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6144672523043687\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6252479710562552\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6208049367395215\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6245415272694431\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6353125228167766\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6158257801284944\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6234449304352063\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6093438741225402\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6180282331643898\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.6088533606203173\n",
      "Epoch 3/3 - Loss: 0.6137438226433028 | Accuracy: 0.6922967189728959 | Sensitivity: 0.8650273224043716 | Specificity: 0.6312741312741312 | Precision: 0.45319209848267966 | F1: 0.5947773811760286 | AUROC: -0.8362480747726693 | AUPR: -0.6350573912583177\n",
      "\n",
      "Fitting estimator 7/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.68849719528368\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6867902607362865\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6827732269428262\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6798783749284123\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6770686987098591\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6750737456689028\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6673990221524352\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.668677355882766\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.667335911711282\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6675009751498778\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6676889565278296\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.660126516164906\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6604159513776247\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6528047265314895\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6892653388710439\n",
      "Epoch 1/3 - Loss: 0.656008244934148 | Accuracy: 0.6751783166904423 | Sensitivity: 0.8733974358974359 | Specificity: 0.602958349552355 | Precision: 0.4448979591836735 | F1: 0.5895078420767983 | AUROC: -0.8266900844387197 | AUPR: -0.627116118692368\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6467882056727681\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6550867336791053\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6493480928615991\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6471501605540454\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6460500793133517\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.645951461492391\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6334220692050684\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.640679790076054\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6412987207048363\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6441152262293927\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6469039978553102\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6353031216922862\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6385167299352357\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6274091598792704\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6905033107013114\n",
      "Epoch 2/3 - Loss: 0.6330696501786023 | Accuracy: 0.6784593437945792 | Sensitivity: 0.8680555555555556 | Specificity: 0.6093810821331257 | Precision: 0.4474118942731278 | F1: 0.5904796511627908 | AUROC: -0.8287893040958422 | AUPR: -0.628832918946827\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6209724367510687\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6351062144884354\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6290694639453488\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6263936264586717\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6265924107478212\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6273642036579372\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6114707577409703\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6227666801577982\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6245659508369427\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6291523506622144\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6336660812613335\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6189866973138289\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6245581184009492\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6105395099576247\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.6938591950255507\n",
      "Epoch 3/3 - Loss: 0.6180792714612319 | Accuracy: 0.6850213980028531 | Sensitivity: 0.8675213675213675 | Specificity: 0.6185286103542235 | Precision: 0.453125 | F1: 0.5953079178885631 | AUROC: -0.830706163727277 | AUPR: -0.6304714850668903\n",
      "\n",
      "Fitting estimator 8/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6909593411963225\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6873963655850547\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.684382731543152\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6815981692337573\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6761968941808354\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6792858214903993\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6762359424338787\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6710376684843539\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6689251545151531\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6695239384465353\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6663485490310029\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6608401342590403\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6592632076154981\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6563080225559441\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6340532131012236\n",
      "Epoch 1/3 - Loss: 0.6590421272804935 | Accuracy: 0.6633380884450785 | Sensitivity: 0.8745980707395499 | Specificity: 0.5867029548989113 | Precision: 0.4342735497605109 | F1: 0.5803698435277382 | AUROC: -0.8256785499375744 | AUPR: -0.6120381243609524\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6648892797747564\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6542169847908249\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6524416072956595\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6522981069734637\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6417344713250994\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6567900711012687\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6517497333729885\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6437567714356985\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6427695630786175\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6478198740928989\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6430979901756405\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6357984238894792\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6356359569699275\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6329260281083784\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.5928121055445046\n",
      "Epoch 2/3 - Loss: 0.6377358435168451 | Accuracy: 0.671469329529244 | Sensitivity: 0.8703108252947481 | Specificity: 0.5993390357698289 | Precision: 0.4407055630936228 | F1: 0.5851197982345523 | AUROC: -0.8275171314794124 | AUPR: -0.6134213333515476\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6491605241355701\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6331567886920013\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6317924213078241\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6346566610836623\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6190217291649232\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6432580253477107\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6358748436281946\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6259741997356661\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6255851426687202\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6343826794461931\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6275962945982133\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6190435741315445\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.619872593333554\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6173029130690302\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.5625630300122645\n",
      "Epoch 3/3 - Loss: 0.6236591535219812 | Accuracy: 0.679172610556348 | Sensitivity: 0.8681672025723473 | Specificity: 0.6106143079315708 | Precision: 0.44714325144907535 | F1: 0.5902714519948988 | AUROC: -0.8292914335101897 | AUPR: -0.614781519620594\n",
      "\n",
      "Fitting estimator 9/9\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6897866082277999\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6868178462529417\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6796945002796679\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6786296432885046\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.6786725158272665\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6753389335700986\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6767714909970302\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.6693960280973158\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6670564077166121\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6646429445989256\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6593598045734468\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.655355420336423\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6607480142032769\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6511243378962613\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6628735759263524\n",
      "Epoch 1/3 - Loss: 0.6550149369211642 | Accuracy: 0.6701854493580599 | Sensitivity: 0.882762312633833 | Specificity: 0.5929599377674056 | Precision: 0.44067343666488507 | F1: 0.5878787878787878 | AUROC: -0.8285710448529411 | AUPR: -0.6185435708290747\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6538770133636284\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6536760198587163\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6375815490037462\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6443658732297938\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6511828538339719\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6466208763130203\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6547821349970768\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6407441649579216\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6391017357966066\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6386342615587166\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6318125311251329\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6265471096443038\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6391576660719999\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.625213018760757\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6470679814397495\n",
      "Epoch 2/3 - Loss: 0.6319458627621792 | Accuracy: 0.6770328102710413 | Sensitivity: 0.880085653104925 | Specificity: 0.603267211201867 | Precision: 0.44625407166123776 | F1: 0.5922190201729106 | AUROC: -0.8300384706040091 | AUPR: -0.6201137660244143\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6317640691466699\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6342953253567566\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6116052334991695\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6230797622631896\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6345816962868897\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.6285848150576\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.6417306720118876\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6227001215259268\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6209553273762928\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6225781264997526\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6143789994393486\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6072991092709583\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6254121362717385\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6082599266640796\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.6385691617486351\n",
      "Epoch 3/3 - Loss: 0.6170915408639733 | Accuracy: 0.6840228245363766 | Sensitivity: 0.8790149892933619 | Specificity: 0.6131855309218203 | Precision: 0.45221702010465437 | F1: 0.5971994908165121 | AUROC: -0.8317056828053306 | AUPR: -0.6219461577139617\n",
      "\n",
      "Fitting meta learner\n",
      "Epoch 1/3 | Batch 1/14 - Loss: 0.6922327277457733\n",
      "Epoch 1/3 | Batch 2/14 - Loss: 0.6914454180202804\n",
      "Epoch 1/3 | Batch 3/14 - Loss: 0.6909010204878475\n",
      "Epoch 1/3 | Batch 4/14 - Loss: 0.6901665554581186\n",
      "Epoch 1/3 | Batch 5/14 - Loss: 0.688658018270415\n",
      "Epoch 1/3 | Batch 6/14 - Loss: 0.6887562067853892\n",
      "Epoch 1/3 | Batch 7/14 - Loss: 0.6874834524988473\n",
      "Epoch 1/3 | Batch 8/14 - Loss: 0.687696422870506\n",
      "Epoch 1/3 | Batch 9/14 - Loss: 0.6855433062171719\n",
      "Epoch 1/3 | Batch 10/14 - Loss: 0.6855682819078504\n",
      "Epoch 1/3 | Batch 11/14 - Loss: 0.6848605676332501\n",
      "Epoch 1/3 | Batch 12/14 - Loss: 0.6844467478460572\n",
      "Epoch 1/3 | Batch 13/14 - Loss: 0.6834842401428952\n",
      "Epoch 1/3 | Batch 14/14 - Loss: 0.6836039333374504\n",
      "Epoch 1/3 | Batch 15/14 - Loss: 0.6775738101339284\n",
      "Epoch 1/3 - Loss: 0.6828244820238857 | Accuracy: 0.73509272467903 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.172455756385563 | AUPR: -0.15986009007663313\n",
      "Epoch 2/3 | Batch 1/14 - Loss: 0.6812671262355714\n",
      "Epoch 2/3 | Batch 2/14 - Loss: 0.6810835036976132\n",
      "Epoch 2/3 | Batch 3/14 - Loss: 0.6815726775198836\n",
      "Epoch 2/3 | Batch 4/14 - Loss: 0.6807768441509099\n",
      "Epoch 2/3 | Batch 5/14 - Loss: 0.6776033760188378\n",
      "Epoch 2/3 | Batch 6/14 - Loss: 0.6797537270991322\n",
      "Epoch 2/3 | Batch 7/14 - Loss: 0.6775871904008695\n",
      "Epoch 2/3 | Batch 8/14 - Loss: 0.6793827284763544\n",
      "Epoch 2/3 | Batch 9/14 - Loss: 0.6752424002740645\n",
      "Epoch 2/3 | Batch 10/14 - Loss: 0.6764297348454058\n",
      "Epoch 2/3 | Batch 11/14 - Loss: 0.6758429180280536\n",
      "Epoch 2/3 | Batch 12/14 - Loss: 0.6757891986078738\n",
      "Epoch 2/3 | Batch 13/14 - Loss: 0.6746355105194236\n",
      "Epoch 2/3 | Batch 14/14 - Loss: 0.6755567528108425\n",
      "Epoch 2/3 | Batch 15/14 - Loss: 0.6640154934013562\n",
      "Epoch 2/3 - Loss: 0.6740763493544667 | Accuracy: 0.73509272467903 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.17245512936872412 | AUPR: -0.1598599996308789\n",
      "Epoch 3/3 | Batch 1/14 - Loss: 0.6719311743223648\n",
      "Epoch 3/3 | Batch 2/14 - Loss: 0.6722729671136328\n",
      "Epoch 3/3 | Batch 3/14 - Loss: 0.6737325014366163\n",
      "Epoch 3/3 | Batch 4/14 - Loss: 0.6728444182037802\n",
      "Epoch 3/3 | Batch 5/14 - Loss: 0.6681314652902\n",
      "Epoch 3/3 | Batch 6/14 - Loss: 0.672151734386288\n",
      "Epoch 3/3 | Batch 7/14 - Loss: 0.669174553578649\n",
      "Epoch 3/3 | Batch 8/14 - Loss: 0.6724296567260052\n",
      "Epoch 3/3 | Batch 9/14 - Loss: 0.6664282699870211\n",
      "Epoch 3/3 | Batch 10/14 - Loss: 0.6686773172316394\n",
      "Epoch 3/3 | Batch 11/14 - Loss: 0.6682146405877972\n",
      "Epoch 3/3 | Batch 12/14 - Loss: 0.6684638964405201\n",
      "Epoch 3/3 | Batch 13/14 - Loss: 0.6671268120089565\n",
      "Epoch 3/3 | Batch 14/14 - Loss: 0.6687883589974085\n",
      "Epoch 3/3 | Batch 15/14 - Loss: 0.652189283996474\n",
      "Epoch 3/3 - Loss: 0.6666593803253904 | Accuracy: 0.73509272467903 | Sensitivity: 0.0 | Specificity: 1.0 | Precision: 1 | F1: 0.0 | AUROC: -0.17245502486591768 | AUPR: -0.15986000121011784\n",
      "{'accuracy': 0.73509272467903, 'sensitivity': 0.0, 'specificity': 1.0, 'precision': 1, 'f1': 0.0, 'auroc': -0.17245502486591768, 'aupr': -0.15986000121011784}\n"
     ]
    }
   ],
   "source": [
    "tp = TelcoPreprocessor('./Telco-Customer-Churn.csv', 'Churn')\n",
    "x, y = tp.preprocess_data()\n",
    "print(x.shape, y.shape)\n",
    "print(y[:10])\n",
    "\n",
    "logistic_regressor = LogisticRegressor(x, y)\n",
    "mean_ensembler = MeanEnsembler(x, y)\n",
    "voting_ensembler = VotingEnsembler(x, y)\n",
    "stacking_ensembler = StackingEnsembler(x, y)\n",
    "\n",
    "# print(logistic_regressor.fit(lr = 0.01, epoch = 3, batch_size = 500, verbose=True))\n",
    "# print(mean_ensembler.fit(lr = 0.01, epoch = 3, batch_size = 500, verbose=True))\n",
    "# print(voting_ensembler.fit(lr = 0.01, epoch = 3, batch_size = 500, verbose=True))\n",
    "print(stacking_ensembler.fit(lr = 0.01, epoch = 3, batch_size = 500, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
